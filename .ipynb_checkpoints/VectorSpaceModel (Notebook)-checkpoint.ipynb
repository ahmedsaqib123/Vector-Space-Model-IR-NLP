{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Space Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach for Making Vector Space Model \n",
    "#1.Preprocessing -Removal of stopwords, lemmatization, tokenization\n",
    "#2.TF-IDF Score \n",
    "#3.Query Processing.\n",
    "#4. Cosine Similarity\n",
    "#5. Ranking of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries \n",
    "import nltk #Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "# Step1. Importing all the stopwords from the file for the removal. \n",
    "\n",
    "with open('Stopword-List.txt','r') as lines:\n",
    "    stopwords = lines.readlines()\n",
    "    stopwords = [x.rstrip() for x in stopwords]\n",
    "    print(stopwords)\n",
    "    \n",
    "# with-open automatically closes the file. \n",
    "# stop words are read and then stripped of, stored in stopwords.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2. Tokenization - Removal of Stopwords \n",
    "documents = {}\n",
    "docs = []\n",
    "for x in range(1,51):\n",
    "    myfile=open(\"ShortStories/\"+str(x)+ \".txt\",\"r\",encoding='utf-8')\n",
    "    #for x in tokens:\n",
    "    #print(x)\n",
    "    \n",
    "    words=myfile.read().replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"'\",\"\").replace(\"]\",\" \").replace(\"[\",\"\").replace(\",\",\" \").replace(\"?\",\"\").replace(\"\\n\",\" \").replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"!\",\"\").replace(\"“\",\"\").replace(\"”\",\"\").replace(\":\",\"\").replace(\"*\",\"\").replace(\"—\",\"\").replace(\";\",\"\").replace(\"’\",\"\").split()\n",
    "    \n",
    "    #for i in range(len(temp)):\n",
    "    #temp[i]=[p_stemmer.stem(x.lower()) for x in temp[i]]\n",
    "    lema=[lemmatizer.lemmatize(x.lower()) for x in words]\n",
    "    tokens = [x for x in words if x not in stopwords]\n",
    "    docs.append(tokens)\n",
    "    documents[x]=tokens\n",
    "    \n",
    "    \n",
    "myfile.close()\n",
    "\n",
    "#len(documents)\n",
    "#len(documents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the entire dataset into a single corpus.\n",
    "corpus = []\n",
    "def makecorpus(docs):\n",
    "    for i in docs:\n",
    "        if type(i) == list:\n",
    "            makecorpus(i)\n",
    "        else:\n",
    "            corpus.append(i)\n",
    "            \n",
    "makecorpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90329"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10421"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique identification \n",
    "corpus = list(set(corpus))\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentVectors={}\n",
    "\n",
    "for x in range(1,51):\n",
    "    documentVectors[x]=dict.fromkeys(corpus,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,51):\n",
    "    for word in documents[x]:\n",
    "        documentVectors[x][word]+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of term-frequency --tf--\n",
    "tf = {}\n",
    "for x in range(1,51):\n",
    "    tf[x]={}\n",
    "    for word,count in documentVectors[x].items():\n",
    "        tf[x][word]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[23]['across']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document wise unique tokens.\n",
    "for x in range(1,50):\n",
    "    documents[x]=set(documents[x])\n",
    "    documents[x]=list(set(documents[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDocCount=dict.fromkeys(corpus,0)\n",
    "for word in corpus:\n",
    "    for x in range(1,51):\n",
    "        if word in documents[x]:\n",
    "            WordDocCount[word]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordDocCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation inverse document frequency --idf-- \n",
    "import math\n",
    "idf = {}\n",
    "for word in corpus:\n",
    "    if WordDocCount[word] > 0:\n",
    "        df=WordDocCount[word]\n",
    "        if df > 50:\n",
    "            df=50\n",
    "    idf[word]=math.log(50/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tf-idf score\n",
    "tfidf={}\n",
    "for x in range(1,50):\n",
    "    tfidf[x]={}\n",
    "    for word in documentVectors[x]:\n",
    "        tfidf[x][word]=tf[x][word]*idf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ENTER QUERY FOR SEARCHING:  really love painter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'love', 'painter']\n"
     ]
    }
   ],
   "source": [
    "# Query Processing \n",
    "\n",
    "def preprocessing_query(query):\n",
    "    #Preprocessing of the Query. \n",
    "    query=query.replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"'\",\"\").replace(\"]\",\" \").replace(\"[\",\"\").replace(\",\",\" \").replace(\"?\",\"\").replace(\"\\n\",\" \").replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"!\",\"\").replace(\"“\",\"\").replace(\"”\",\"\").replace(\":\",\"\").replace(\"/\",\" / \").replace(\"*\",\"\").replace(\"—\",\"\").replace(\";\",\"\").replace(\"’\",\"\").split()\n",
    "    lem_query=[lemmatizer.lemmatize(x.lower()) for x in query]\n",
    "    return lem_query\n",
    "\n",
    "\n",
    "query = input(\"ENTER QUERY FOR SEARCHING: \")\n",
    "x =preprocessing_query(query)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVector = dict.fromkeys(corpus,0)\n",
    "for word in x:\n",
    "    queryVector[word]+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in queryVector:\n",
    "    queryVector[words]=queryVector[words]*idf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarityy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "cosine = {}\n",
    "temp=0\n",
    "vector1=np.array([list(queryVector.values())])\n",
    "for x in range(1,50):\n",
    "    vector2=np.array([list(tfidf[x].values())])\n",
    "    if cosine_similarity(vector1,vector2)>0.005: #alpha value.\n",
    "        temp=cosine_similarity(vector1,vector2)[0][0]\n",
    "        cosine[x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  24\n",
      "\n",
      "Documents  Cosine Values\n",
      "\n",
      "26 0.05779304093936671\n",
      "30 0.0352883275155501\n",
      "28 0.019605256592269964\n",
      "21 0.019249115058293565\n",
      "18 0.01681704209905132\n",
      "31 0.01429678132246556\n",
      "23 0.014165205056825938\n",
      "19 0.014085133772228007\n",
      "22 0.0138418016927828\n",
      "37 0.01242525229593101\n",
      "4 0.010957194377987266\n",
      "24 0.01019227930035746\n",
      "2 0.009990302865230513\n",
      "25 0.00968504663153082\n",
      "46 0.009028012404799451\n",
      "8 0.008970166783708615\n",
      "14 0.008748800147718557\n",
      "16 0.00873244070219261\n",
      "38 0.008712047210118801\n",
      "1 0.007361246957309069\n",
      "40 0.007271764004919895\n",
      "45 0.005701060521085083\n",
      "5 0.005429129377836125\n",
      "27 0.005151590767497743\n"
     ]
    }
   ],
   "source": [
    "sort_orders = sorted(cosine.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Length: ' , len(sort_orders))\n",
    "print('\\nDocuments  Cosine Values\\n')\n",
    "for i in sort_orders:\n",
    "\tprint(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
